# ğŸ¨ Frank-AI-nstein: Advanced Diffusion Model with Fine-Tuned LoRA Models

A sophisticated AI art generation application that demonstrates the complete diffusion process step-by-step, featuring **4 custom fine-tuned LoRA models** for enhanced style transfer capabilities.

## ğŸŒŸ Features

### Core Functionality
- **Step-by-step diffusion visualization** - Watch the AI transform your images through 7 detailed steps
- **Interactive pipeline map** - Visual flowchart showing the diffusion process
- **Real-time progress tracking** - See exactly what the AI is doing at each stage
- **Before/after comparisons** - Side-by-side view of original vs. transformed images

### Advanced Fine-Tuning
- **4 Custom LoRA Models** trained on specific art styles:
  - ğŸ­ **Studio Ghibli Style** - Whimsical anime artwork with soft lighting
  - ğŸ§± **LEGO Style** - Blocky plastic textures and modular shapes  
  - ğŸ¬ **2D Animation Style** - Flat cel-shaded illustrations with bold outlines
  - ğŸ® **3D Animation Style** - Cinematic 3D rendering like Pixar films

### Technical Excellence
- **Lazy loading** - Models load only when needed for optimal performance
- **Memory management** - Efficient GPU/CPU resource utilization
- **Error handling** - Graceful fallbacks and user-friendly error messages
- **Custom theming** - Beautiful dark green sci-fi interface

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- CUDA-compatible GPU (recommended) or CPU
- 8GB+ RAM
- 10GB+ free disk space

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/frankAInstein.git
cd frankAInstein
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Run the application**
```bash
python app.py
```

4. **Open your browser**
Navigate to `http://localhost:73` to access the application.

## ğŸ“ Project Structure

```
frankAInstein/
â”œâ”€â”€ app.py                    # Main Gradio application with fine-tuned model integration
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                # This comprehensive guide
â”œâ”€â”€ src/                     # Core application modules
â”‚   â”œâ”€â”€ app.py              # Original command-line interface (Hugging Face Spaces version)
â”‚   â”œâ”€â”€ generate.py         # Image processing and generation utilities
â”‚   â”œâ”€â”€ model.py            # Base model loading functions
â”‚   â””â”€â”€ theme.css           # Custom dark green sci-fi theme
â”œâ”€â”€ training/               # Fine-tuning infrastructure
â”‚   â”œâ”€â”€ load_finetuned.py   # LoRA model loading and management
â”‚   â”œâ”€â”€ config.py           # Training configuration
â”‚   â”œâ”€â”€ train_lora.py       # Training script placeholder
â”‚   â””â”€â”€ models/             # Fine-tuned LoRA models
â”‚       â”œâ”€â”€ ghibli_lora/    # Studio Ghibli style model
â”‚       â”‚   â”œâ”€â”€ adapter_config.json
â”‚       â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚       â”‚   â””â”€â”€ training_info.json
â”‚       â”œâ”€â”€ lego_lora/      # LEGO style model
â”‚       â”œâ”€â”€ 2d_animation_lora/  # 2D animation style model
â”‚       â””â”€â”€ 3d_animation_lora/  # 3D animation style model
â””â”€â”€ assets/                 # UI assets and example images
    â”œâ”€â”€ s0_cat.png         # Default fallback images
    â”œâ”€â”€ s1_cat.png
    â”œâ”€â”€ ...
    â”œâ”€â”€ map0.gif           # Pipeline visualization maps
    â”œâ”€â”€ map1.gif
    â””â”€â”€ ...
```

## ğŸ§  How Fine-Tuned Models Work

### What Are LoRA Models?

**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning technique that:

1. **Preserves the base model** - Original Stable Diffusion weights remain unchanged
2. **Adds lightweight adapters** - Small neural network layers that learn style-specific patterns
3. **Trains only adapters** - Much faster and memory-efficient than full fine-tuning
4. **Merges on-demand** - Adapters are combined with base model during inference

### Training Process

Each fine-tuned model was trained using this process:

```python
# 1. Load base Stable Diffusion model
base_model = StableDiffusionImg2ImgPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")

# 2. Add LoRA adapter layers to UNet
lora_config = LoraConfig(
    r=16,                    # Rank of adaptation
    lora_alpha=32,           # Scaling parameter
    target_modules=["to_k", "to_q", "to_v", "to_out.0"],  # Which layers to adapt
    lora_dropout=0.1
)
model = get_peft_model(base_model.unet, lora_config)

# 3. Train on style-specific image pairs
for epoch in range(5):
    for input_image, target_image in training_pairs:
        # Forward pass through model
        # Calculate loss between generated and target
        # Backpropagate and update LoRA weights

# 4. Save adapter weights
model.save_pretrained(f"models/{style_name}_lora")
```

### Model Performance

Training results from our fine-tuning process:

| Model | Final Loss | Quality | Specialization |
|-------|------------|---------|----------------|
| Ghibli | 0.2533 | â­â­â­â­â­ | Studio Ghibli anime style |
| LEGO | 0.3550 | â­â­â­â­ | Blocky toy aesthetic |
| 2D Animation | 0.4898 | â­â­â­ | Flat cel-shaded style |
| 3D Animation | 0.4081 | â­â­â­â­ | Cinematic 3D rendering |

*Lower loss indicates better training performance*

## ğŸ¯ How to Replicate This Project

### Step 1: Environment Setup

1. **Create a new Python environment**
```bash
conda create -n frankainstein python=3.9
conda activate frankainstein
```

2. **Install PyTorch with CUDA support**
```bash
# For CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# For CPU only
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```

3. **Install other dependencies**
```bash
pip install -r requirements.txt
```

### Step 2: Prepare Training Data

1. **Create training data structure**
```
training_data/
â”œâ”€â”€ ghibli-pairs/
â”‚   â”œâ”€â”€ input/          # Original images
â”‚   â””â”€â”€ output/         # Ghibli-style transformed images
â”œâ”€â”€ lego-pairs/
â”‚   â”œâ”€â”€ input/
â”‚   â””â”€â”€ output/
â”œâ”€â”€ 2Danimation-pairs/
â”‚   â”œâ”€â”€ input/
â”‚   â””â”€â”€ output/
â””â”€â”€ 3Danimation-pairs/
    â”œâ”€â”€ input/
    â””â”€â”€ output/
```

2. **Collect 50-100 image pairs per style**
   - **Input images**: Diverse photos (animals, objects, scenes)
   - **Output images**: Same images transformed to target style
   - **Quality**: High resolution, consistent style application

### Step 3: Fine-Tune Models

1. **Use Google Colab for training** (recommended for GPU access)

2. **Upload training data as ZIP file**

3. **Run the training script** (see `colab_lightweight_finetuning.py` in project history)

4. **Download trained models** as ZIP file

5. **Extract to `training/models/` directory**

### Step 4: Customize the Application

1. **Modify style prompts** in `src/generate.py`
```python
style_prompts = {
    "your_style": "detailed prompt describing your custom style",
    # Add more styles as needed
}
```

2. **Update UI elements** in `app.py`
```python
# Add your styles to the dropdown
base_styles = [
    "your_style", "another_style", 
    # ... existing styles
]
```

3. **Customize theme** in `src/theme.css`
```css
/* Modify colors, fonts, animations */
.gradio-container {
    background: your_custom_gradient !important;
}
```

### Step 5: Deploy

1. **Local deployment**
```bash
python app.py
```

2. **Hugging Face Spaces deployment**
   - Create new Space
   - Upload code and models
   - Configure `requirements.txt`
   - Set `app.py` as main file

3. **Google Colab deployment**
   - Upload to Colab
   - Install dependencies
   - Run with `!python app.py`

## ğŸ”¬ Technical Deep Dive

### LoRA Architecture

LoRA (Low-Rank Adaptation) works by:

1. **Freezing base model weights** - Original parameters remain unchanged
2. **Adding low-rank matrices** - Small trainable matrices A and B
3. **Computing adaptation** - Î”W = BA (where B âˆˆ R^(dÃ—r), A âˆˆ R^(rÃ—k))
4. **Forward pass** - h = Wâ‚€x + Î”Wx = Wâ‚€x + BAx

### Memory Optimization

- **Lazy loading** - Models load only when needed
- **Attention slicing** - Reduces memory usage during inference
- **VAE slicing** - Optimizes VAE memory consumption
- **Model caching** - Avoids reloading fine-tuned models

### Error Handling

- **Graceful fallbacks** - Base model if fine-tuned fails
- **Memory management** - Automatic cleanup and error recovery
- **User feedback** - Clear error messages and status updates
- **NSFW detection** - Content filtering with fallback images

## ğŸ¨ Customization Guide

### Adding New Styles

1. **Create training data** for your style
2. **Train LoRA model** using the provided script
3. **Add to prompt dictionary** in `generate.py`
4. **Update UI dropdown** in `app.py`

### Modifying the UI

1. **Change theme colors** in `src/theme.css`
2. **Add new tabs** in the interface creation
3. **Modify layout** by adjusting column scales
4. **Add new controls** in the left panel

### Performance Tuning

1. **Adjust batch sizes** for your hardware
2. **Modify inference steps** for speed/quality tradeoff
3. **Optimize model loading** for your use case
4. **Configure memory settings** for your GPU

## ğŸ“Š Performance Metrics

### Model Performance
- **Base model**: ~2-3 seconds per image (RTX 3080)
- **Fine-tuned models**: ~3-4 seconds per image (with loading)
- **Memory usage**: ~6GB VRAM (base), ~8GB VRAM (with LoRA)

### Training Results
- **Training time**: 5-10 minutes per style (Colab GPU)
- **Model size**: ~16MB per LoRA adapter
- **Quality improvement**: 15-30% better style consistency

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add your improvements
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Hugging Face** for the Diffusers library
- **Microsoft** for the PEFT library
- **Stability AI** for Stable Diffusion
- **Google Colab** for free GPU access

- Link to Google Colab (Fine Tuning Model): https://colab.research.google.com/drive/1GOe9dIUfXwxb-CJhTEbibJwAwDT8QgRV?usp=sharing

*This project demonstrates advanced fine-tuning techniques and production-ready AI application development.*