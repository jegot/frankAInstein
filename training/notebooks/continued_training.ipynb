{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Summaries for continued LoRa learning:\n",
        "\n",
        "\n",
        "***additional dataset***\n",
        "\n",
        "45 input/output pairs per style || 5 epochs each || 512 sizes || lr=5e-5\n",
        "\n",
        "summary:\n",
        "\n",
        "*   ghibli: Final loss = 0.0916\n",
        "*   lego: Final loss = 0.1126\n",
        "*   2d_animation: Final loss = 0.1057\n",
        "*   3d_animation: Final loss = 0.1360\n",
        "\n",
        "---\n",
        "\n",
        "***additional-additional dataset***\n",
        "\n",
        "**Continued training from models generated post additional-dataset\n",
        "\n",
        "30 input/output pairs per style || 512 size || learning rate & epochs adjusted per style\n",
        "\n",
        "summary:\n",
        "\n",
        "*   ghibli: Final loss = 0.0932 (ran @ 5 epochs and lr=5e-5)\n",
        "*   lego: Final loss = 0.0834 (ran @ 3 epochs and lr=1e-5)\n",
        "*   2d_animation: Final loss = 0.0854 (ran @ 3 epochs and lr=1e-5)\n",
        "*   3d_animation: Final loss = 0.0893 (ran @ 5 epochs and lr=5e-5)\n"
      ],
      "metadata": {
        "id": "JE79FA9P94E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install Dependencies\n",
        "!pip install -q peft accelerate diffusers transformers datasets\n",
        "!pip install -q torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "#Deepseek used for coding assitance"
      ],
      "metadata": {
        "id": "t-Z6-nAApk_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and Setup\n",
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Reduce TensorFlow logging\n",
        "from safetensors.torch import load_file\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ],
      "metadata": {
        "id": "CdlqN84tp-J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Upload and Extract Training Data\n",
        "print(\"Upload your data zip file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Zip of data expected for upload\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        print(f\"Extracting {filename}...\")\n",
        "        # Create a directory named 'data' and extract the zip file into it\n",
        "        os.makedirs('data', exist_ok=True)\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            # List contents before extracting\n",
        "            file_list = zip_ref.namelist()\n",
        "            print(f\"ZIP contains {len(file_list)} files/folders\")\n",
        "            print(\"First few items:\", file_list[:10])\n",
        "            zip_ref.extractall('data')  # Extract into the 'data' directory\n",
        "\n",
        "        print(f\"Extracted {filename} successfully into ./data!\")\n",
        "\n",
        "        expected_styles = ['ghibli-pairs', 'lego-pairs', '2Danimation-pairs', '3Danimation-pairs']\n",
        "\n",
        "        data_path = 'data'\n",
        "        if os.path.exists(data_path):\n",
        "            data_dirs = [d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))]\n",
        "            print(f\"Found directories inside '{data_path}': {data_dirs}\")\n",
        "\n",
        "            # Check each expected style\n",
        "            for style in expected_styles:\n",
        "                style_path = os.path.join(data_path, style)\n",
        "                input_path = os.path.join(style_path, 'input')\n",
        "                output_path = os.path.join(style_path, 'output')\n",
        "\n",
        "                if os.path.exists(input_path) and os.path.exists(output_path):\n",
        "                    input_files = len([f for f in os.listdir(input_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                    output_files = len([f for f in os.listdir(output_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
        "                    print(f\"{style}: {input_files} input files, {output_files} output files\")\n",
        "                else:\n",
        "                    print(f\"{style}: Missing input or output directory within '{style_path}'\")\n",
        "        else:\n",
        "            print(f\"Directory '{data_path}' not found after extraction\")\n",
        "    else:\n",
        "        print(f\"Skipped {filename} - not a ZIP file\")"
      ],
      "metadata": {
        "id": "IztsNu7tqNmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Dataset Class\n",
        "class ImagePairDataset(Dataset):\n",
        "    # max pairs adjusted per dataset\n",
        "    def __init__(self, input_dir, output_dir, transform=None, max_pairs=30):\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.transform = transform\n",
        "        self.max_pairs = max_pairs\n",
        "\n",
        "        # Get matching image pairs\n",
        "        input_files = [f for f in os.listdir(input_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        output_files = [f for f in os.listdir(output_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        self.pairs = []\n",
        "        for input_file in input_files:\n",
        "            if input_file in output_files:\n",
        "                self.pairs.append(input_file)\n",
        "\n",
        "        # Limit to max_pairs for training\n",
        "        if len(self.pairs) > max_pairs:\n",
        "            self.pairs = random.sample(self.pairs, max_pairs)\n",
        "\n",
        "        print(f\"Using {len(self.pairs)} image pairs for training from {input_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.pairs[idx]\n",
        "\n",
        "        input_path = os.path.join(self.input_dir, filename)\n",
        "        output_path = os.path.join(self.output_dir, filename)\n",
        "\n",
        "        input_image = Image.open(input_path).convert('RGB')\n",
        "        output_image = Image.open(output_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            input_image = self.transform(input_image)\n",
        "            output_image = self.transform(output_image)\n",
        "\n",
        "        return input_image, output_image"
      ],
      "metadata": {
        "id": "-YyjoVGEqTg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Upload existing LoRA .safetensors files...\")\n",
        "uploaded_loras = files.upload()\n",
        "\n",
        "# Check what files were uploaded\n",
        "for filename in uploaded_loras.keys():\n",
        "    print(f\"Uploaded: {filename}\")\n",
        "\n",
        "existing_loras = {}\n",
        "for style in ['ghibli', 'lego', '2d_animation', '3d_animation']:\n",
        "    possible_filenames = [\n",
        "        f\"{style}_lora_proplus.safetensors\",\n",
        "        f\"models/{style}/adapter_model.safetensors\"\n",
        "    ]\n",
        "\n",
        "    for filename in possible_filenames:\n",
        "        if os.path.exists(filename):\n",
        "            existing_loras[style] = filename\n",
        "            print(f\"Found existing LoRA for {style}: {filename}\")\n",
        "            break\n",
        "    else:\n",
        "        print(f\"No existing LoRA found for {style}\")"
      ],
      "metadata": {
        "id": "z87ruJ1KNaH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: U-Net Fine-tuning Function\n",
        "def lightweight_finetune_style(style_name, input_dir, output_dir, device=\"cuda\", existing_lora_path=None):\n",
        "\n",
        "    print(f\"\\nStarting optimized fine-tuning for {style_name}...\")\n",
        "\n",
        "    if existing_lora_path:\n",
        "        print(f\"Continuing training from existing LoRA: {existing_lora_path}\")\n",
        "\n",
        "    if not os.path.exists(input_dir) or not os.path.exists(output_dir):\n",
        "        print(f\"Directories not found: {input_dir} or {output_dir}\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"Loading Stable Diffusion 1.5 components...\")\n",
        "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "    # Load individual components (let autocast handle precision)\n",
        "    tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
        "    text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\").to(device)\n",
        "    vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(device)\n",
        "    unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\").to(device)\n",
        "    scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
        "\n",
        "    # Freeze all components except U-Net\n",
        "    text_encoder.requires_grad_(False)\n",
        "    vae.requires_grad_(False)\n",
        "\n",
        "    print(\"Loaded all model components\")\n",
        "\n",
        "    # Configure LoRA for U-Net only\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "    )\n",
        "\n",
        "    # Apply LoRA to U-Net only\n",
        "    unet = get_peft_model(unet, lora_config)\n",
        "\n",
        "    if existing_lora_path and os.path.exists(existing_lora_path):\n",
        "        print(f\"Loading existing LoRA weights from {existing_lora_path}\")\n",
        "        try:\n",
        "            if existing_lora_path.endswith('.safetensors'):\n",
        "                state_dict = load_file(existing_lora_path, device=device)\n",
        "                unet.load_state_dict(state_dict, strict=False)\n",
        "                print(\"Successfully loaded existing LoRA weights from .safetensors\")\n",
        "            elif existing_lora_path.endswith('.pth'):\n",
        "                state_dict = torch.load(existing_lora_path, map_location=device)\n",
        "                if 'model_state_dict' in state_dict:\n",
        "                    unet.load_state_dict(state_dict['model_state_dict'], strict=False)\n",
        "                else:\n",
        "                    unet.load_state_dict(state_dict, strict=False)\n",
        "                print(\"Successfully loaded existing LoRA weights from .pth\")\n",
        "            else:\n",
        "                print(f\"Unsupported file format: {existing_lora_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load existing LoRA: {e}\")\n",
        "    else:\n",
        "        print(\"Starting with fresh LoRA weights\")\n",
        "\n",
        "    unet.print_trainable_parameters()\n",
        "\n",
        "    # Full 512px resolution\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5], [0.5])\n",
        "    ])\n",
        "\n",
        "    # Create dataset - use all available pairs\n",
        "    dataset = ImagePairDataset(input_dir, output_dir, transform=transform, max_pairs=50)\n",
        "\n",
        "    if len(dataset) == 0:\n",
        "        print(f\"No training pairs found for {style_name}\")\n",
        "        return None, None\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # Training parameters for continued training\n",
        "    num_epochs = 3\n",
        "    optimizer = torch.optim.AdamW(unet.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
        "\n",
        "    # Training setup\n",
        "    accumulation_steps = 2\n",
        "    unet.train()\n",
        "    losses = []\n",
        "\n",
        "    # Early stopping setup\n",
        "    best_loss = float('inf')\n",
        "    patience = 2\n",
        "    patience_counter = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = f\"checkpoints/{style_name}\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Starting diffusion training ({num_epochs} epochs, {len(dataset)} pairs, 512px)...\")\n",
        "\n",
        "    # Training loop with mixed precision and checkpointing\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, (input_images, output_images) in enumerate(progress_bar):\n",
        "            input_images = input_images.to(device)\n",
        "            output_images = output_images.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                latents = vae.encode(output_images).latent_dist.sample() * 0.18215\n",
        "\n",
        "            noise = torch.randn_like(latents)\n",
        "            batch_size = latents.shape[0]\n",
        "            timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "            noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "            prompt = [f\"high quality image in {style_name} style, detailed, artistic\"] * batch_size\n",
        "            text_inputs = tokenizer(\n",
        "                prompt,\n",
        "                padding=\"max_length\",\n",
        "                max_length=77,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                encoder_hidden_states = text_encoder(text_inputs.input_ids).last_hidden_state\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
        "                loss = F.mse_loss(noise_pred, noise) / accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            epoch_loss += loss.item() * accumulation_steps\n",
        "            progress_bar.set_postfix({\"batch_loss\": f\"{loss.item() * accumulation_steps:.4f}\"})\n",
        "\n",
        "            del input_images, output_images, latents, noise, noisy_latents, noise_pred\n",
        "            if batch_idx % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Update learning rate\n",
        "        lr_scheduler.step()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.2e}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "            best_model_state = unet.state_dict().copy()\n",
        "            print(f\"New best loss: {best_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        checkpoint_path = f\"{checkpoint_dir}/epoch_{epoch+1}.pth\"\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': unet.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': avg_loss,\n",
        "            'lora_config': lora_config.to_dict(),\n",
        "        }, checkpoint_path)\n",
        "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch+1}\")\n",
        "            # Restore best model\n",
        "            if best_model_state is not None:\n",
        "                unet.load_state_dict(best_model_state)\n",
        "            break\n",
        "\n",
        "    # Save final model\n",
        "    save_dir = f\"models/{style_name}_lora_proplus\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    unet.save_pretrained(save_dir)\n",
        "\n",
        "    # Save training info\n",
        "    training_info = {\n",
        "        \"style\": style_name,\n",
        "        \"training_pairs\": len(dataset),\n",
        "        \"epochs\": num_epochs,\n",
        "        \"final_loss\": losses[-1] if losses else None,\n",
        "        \"best_loss\": best_loss,\n",
        "        \"image_resolution\": 512,\n",
        "        \"lora_rank\": 16,\n",
        "        \"base_model\": \"runwayml/stable-diffusion-v1-5\",\n",
        "        \"training_complete\": True\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(save_dir, \"training_info.json\"), \"w\") as f:\n",
        "        json.dump(training_info, f, indent=2)\n",
        "\n",
        "    print(f\"Training completed for {style_name}\")\n",
        "    print(f\"Final loss: {losses[-1]:.4f}\" if losses else \"No loss recorded\")\n",
        "    print(f\"Best loss: {best_loss:.4f}\")\n",
        "    print(f\"Model saved to: {save_dir}\")\n",
        "\n",
        "    return save_dir, losses"
      ],
      "metadata": {
        "id": "h_2-k8NE2yRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Main Training Loop\n",
        "print(\"U-Net LoRA Fine-tuning for Stable Diffusion 1.5\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def find_style_directories():\n",
        "    \"\"\"Find available style directories from our training data structure\"\"\"\n",
        "    styles = {}\n",
        "    data_path = 'data'\n",
        "\n",
        "    if not os.path.exists(data_path):\n",
        "        print(f\"Directory '{data_path}' not found.\")\n",
        "        return styles\n",
        "\n",
        "    style_mappings = {\n",
        "        'ghibli-pairs': 'ghibli',\n",
        "        'lego-pairs': 'lego',\n",
        "        '2Danimation-pairs': '2d_animation',\n",
        "        '3Danimation-pairs': '3d_animation'\n",
        "    }\n",
        "\n",
        "    for dir_name, style_name in style_mappings.items():\n",
        "        dir_path = os.path.join(data_path, dir_name)\n",
        "        input_dir = os.path.join(dir_path, 'input')\n",
        "        output_dir = os.path.join(dir_path, 'output')\n",
        "\n",
        "        if os.path.exists(input_dir) and os.path.exists(output_dir):\n",
        "            input_files = len([f for f in os.listdir(input_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            output_files = len([f for f in os.listdir(output_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
        "\n",
        "            if input_files > 0 and output_files > 0:\n",
        "                styles[style_name] = {\n",
        "                    \"input_dir\": input_dir,\n",
        "                    \"output_dir\": output_dir\n",
        "                }\n",
        "                print(f\"Found {style_name}: {input_files} input files, {output_files} output files\")\n",
        "            else:\n",
        "                print(f\"{style_name}: No image files found in input/output directories\")\n",
        "        else:\n",
        "            print(f\"{style_name}: Missing directories\")\n",
        "\n",
        "    return styles\n",
        "\n",
        "# Find available styles\n",
        "styles = find_style_directories()\n",
        "\n",
        "if not styles:\n",
        "    print(\"No valid style directories found for training!\")\n",
        "    print(\"Available directories in /content:\", os.listdir('.') if os.path.exists('.') else \"None\")\n",
        "    if os.path.exists('data'):\n",
        "        print(\"Available directories in /content/data:\", os.listdir('data') if os.path.exists('data') else \"None\")\n",
        "else:\n",
        "    print(f\"Training {len(styles)} styles: {list(styles.keys())}\")\n",
        "\n",
        "results = {}\n",
        "\n",
        "existing_loras = {\n",
        "    'ghibli': 'ghibli_lora_proplus.safetensors',\n",
        "    'lego': 'lego_lora_proplus.safetensors',\n",
        "    '2d_animation': '2d_animation_lora_proplus.safetensors',\n",
        "    '3d_animation': '3d_animation_lora_proplus.safetensors'\n",
        "}\n",
        "\n",
        "for style_name, paths in styles.items():\n",
        "    print(f\"\\nTraining {style_name}...\")\n",
        "\n",
        "    existing_lora_path = existing_loras.get(style_name)\n",
        "\n",
        "    if os.path.exists(paths[\"input_dir\"]) and os.path.exists(paths[\"output_dir\"]):\n",
        "        save_dir, losses = lightweight_finetune_style(\n",
        "            style_name,\n",
        "            paths[\"input_dir\"],\n",
        "            paths[\"output_dir\"],\n",
        "            device,\n",
        "            existing_lora_path=existing_lora_path\n",
        "        )\n",
        "        if save_dir:\n",
        "             results[style_name] = {\"save_dir\": save_dir, \"losses\": losses}\n",
        "    else:\n",
        "        print(f\"Skipping {style_name} - directories not found\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nFine-tuning Summary:\")\n",
        "if results:\n",
        "    for style, result in results.items():\n",
        "        print(f\"{style}: Final loss = {result['losses'][-1]:.4f}\")\n",
        "else:\n",
        "    print(\"No models were fine-tuned.\")\n"
      ],
      "metadata": {
        "id": "3TwMkbfrqfgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download models\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_filename = 'proper_unet_lora_models.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    for root, dirs, files_in_dir in os.walk('models'):\n",
        "        for file in files_in_dir:\n",
        "            file_path = os.path.join(root, file)\n",
        "            arcname = os.path.relpath(file_path, 'models')\n",
        "            zipf.write(file_path, os.path.join('models', arcname))\n",
        "\n",
        "print(f\"Zipped model directory as: {zip_filename}\")\n",
        "files.download('proper_unet_lora_models.zip')\n"
      ],
      "metadata": {
        "id": "ZAfYFpOdJavB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}